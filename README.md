<h1 align="center">
<img src="docs/assets/openvideo-logo2.png" width="600">
</h1><br>


[![HuggingFace](https://img.shields.io/badge/%F0%9F%A4%97%20HuggingFace-OpenVideo-yellow)](https://huggingface.co/OpenVideo)
[![ModelScope](https://img.shields.io/badge/ModelScope-OpenVideo-blue)](https://www.modelscope.cn/organization/OpenVideo)
[![PyPi version](https://badgen.net/pypi/v/openvideo/)](https://pypi.org/pypi/openvideo/)
[![PyPI pyversions](https://img.shields.io/badge/dynamic/json?color=blue&label=python&query=info.requires_python&url=https%3A%2F%2Fpypi.org%2Fpypi%2Fdata4co%2Fjson)](https://pypi.python.org/pypi/openvideo/) 
[![Downloads](https://static.pepy.tech/badge/openvideo)](https://pepy.tech/project/openvideo)
[![GitHub stars](https://img.shields.io/github/stars/UmiMarch/OpenVideo.svg?style=social&label=Star&maxAge=8640)](https://GitHub.com/UmiMarch/OpenVideo/stargazers/)

OpenVideo specializes in the domain of text-to-video generation, with the goal of providing high-quality and diverse video datasets to AI researchers globally. In addition, it offers comprehensive tools for data collection, cleaning, and annotation, thereby contributing to the advancement of the artificial intelligence industry.

[ä¸­æ–‡ä¸»é¡µ](./README_ZH.md)

## ğŸ“šDataset


|   Source | Resolution | Hours |  Clips |
| :--------: | :--: | :--: | :---------: |
| Pexels-Raw | 720p | 672h | 106k+ clips |


### Downloadï¼š

From [ModelScope](https://www.modelscope.cn/datasets/OpenVideo/pexel-0808-complete-final-test)ï¼š

```
bash git clone https://user_id:access_token@www.modelscope.cn/datasets/OpenVideo/pexel-0808-complete-final-test.git
```

From [huggingface](https://huggingface.co/datasets/OpenVideo/pexel-0808-complete-final-test)ï¼š

```
bash git clone https://user_id:access_token@huggingface.co/datasets/OpenVideo/pexel-0808-complete-final-test
```

(user_id is your username, and access_token needs to be generated in the settings)


### Script:

```
python ./openvideo/video/preprocess/utils/decode_parquet_file.py --parquet_dir your_parquet_path --save_dir your_save_path
```


## âš¡Tools


You can install this package via PyPI by entering the following command in the terminal:

```bash
$ pip install openvideo
```

Alternatively, you can obtain the latest version from GitHub:

```bash
$ pip install -U https://github.com/UmiMarch/OpenVideo/archive/master.zip # with --user for user install (no root)
```

The dependencies for `OpenVideo` are as follows:

```
huggingface_hub>=0.22.2
tqdm>=4.66.1
wget>=3.2
requests>=2.31.0
aiohttp>=3.9.3
async_timeout>=4.0.3
moviepy>=1.0.3
opencv-python>=4.9.0.80
selenium>=4.19.0
scenedetect>=0.6.3
texttable>=1.7.0
bs4>=0.0.2
```

### Video Download

* Mixkit [https://mixkit.co/free-stock-video/](https://mixkit.co/free-stock-video/)
```python
from openvideo.video.fetch import MixkitVideoFetch
 
mixkit_fetch = MixkitVideoFetch(root_dir="your/video/save/path")
mixkit_fetch.download_with_category_page_idx(
    category="sky", # Video category
    page_idx=1, # Start downloading from this page
    start_idx=22, # Start downloading from this video
    platform="linux" # Running platform
)
```

* Pixabay [https://pixabay.com/zh](https://pixabay.com/zh)
```python
from openvideo.video.fetch import PixabayVideoFetch

pixabay = PixabayVideoFetch("your/video/save/path")
pixabay.download(
    chrome_exe_path=r"your/chrome/exe/path",
    username="your/pixabay/username",
    password="your/pixabay/password",
    headless=False,
    platform="windows" # Currently only supports Windows
)
```

* Pexels [https://www.pexels.com/](https://www.pexels.com/)
```python
from openvideo.video.fetch import PexelsVdieoFetch, PexelsAPI

# Step 1: Call the API to obtain video links
pexels_api = PexelsAPI(
    api="your/pexels/api", 
    save_path="pexels_api.npy"
)
pexels_api.fetch_api(
    start_page=1, # Starting page
    end_page=2, # Ending page
    save_api_dict_every_pages=1 # Save the API dictionary every n pages
)

# Step 2: Download videos
pexels = PexelsVdieoFetch("pexels")
pexels.download(
    api_npy_save_path="pexels_api.npy", 
    chrome_exe_path=r"your/chrome/exe/path",
    headless=False
)
```

### Video Annotation Platform

We have developed a [video annotation platform]((https://huggingface.co/spaces/OpenVideo/GPT4o-Azure-Caption-Pixel)) based on the Rust programming language, designed to efficiently generate labels for various media types, including images and videos. This platform supports the invocation of state-of-the-art AI models such as GPT-4o, Gemini, and Claude3, and offers flexible configuration options. It is designed for high performance, capable of processing 100 queries per second, with task processing capacity scalable to 200 million queries. Utilizing 100 API accounts, this tool can synthesize a dataset containing 200,000 videos within 8 hours. All outputs are categorized and organized by model and prompt, ensuring a clear structure for subsequent research and application integration.

![image-20250123193428569](./docs/assets/caption_platform.png)

(If display issues occur, please try using other browsers. e.g. Edge.)


### Annotation Validation Platform

We provide an [annotation validation platform](https://huggingface.co/spaces/OpenVideo/AIL-Caption-lalala-Dup) where users can view, validate, and modify annotations for already annotated video datasets.

**Usage:**


1. Open the HuggingFace link and enter your [personal token](https://huggingface.co/settings/tokens).

2. Play videos, view the corresponding annotation texts, modify them, than switch to the next video.

   ![openvideo_tagger](./docs/assets/openvideo_tagger.png)


**For custom datasets:**

1. The dataset and code must be on the same platform (e.g., the dataset is hosted on HuggingFace).

2. Modify the dataset path in [run.py](https://huggingface.co/spaces/OpenVideo/AIL-Caption-lalala-Dup/blob/main/run.py#L7).

### Data Migration

We provide a general-purpose [data migration platform](https://huggingface.co/spaces/OpenVideo/HF_To_MS) for transferring datasets from HuggingFace to ModelScope, facilitating access and usage of datasets across different regional networks.

**Usageï¼š**

Enter your HuggingFace access token & dataset path, ModelScope access token & repository directory, then click `Submit` to run the dataset migration from HuggingFace to the corresponding ModelScope repository automatically.

![data_transfer](./docs/assets/data_transfer.png)


## ğŸ‘¨â€ğŸ’» Contributors

Crawling Algorithmsï¼š@yangming @heatingma @ZZY @æ™šæ¥é£é›ª

Video Download: @yangming @æ™šæ¥é£é›ª @æ°æ°æ°

Data Cleaningï¼š@ä¸€é©¬å¹³å·  @zjukop @ä¼Šå°å¸ƒ

Prompt: @Tiger.C @dpyneo @å·§å…‹åŠ›

Labeling: @YUE @zjukop

Validation Platform: @YUE @æ™šæ¥é£é›ª

Data Migration: @æ™šæ¥é£é›ª @heatingma

Manual Validation: @ä¸€é©¬å¹³å· @dpyneo @æ¨å˜‰æ˜Š @flipped @yi @believe @æ€æ©

Project Research: @dingby @believe

Aesthetic Guidance: @å›¾æ‹‰ @æ¨å˜‰æ˜Š

Documentation: @ZZY @æªæª

Project Coordination: @å·§å…‹åŠ›


## ğŸ™Â Acknowledgments

Server/Financial Support: Li Bai AI Lab

Storage: HuggingFace, ModelScope, OPENDataLab

Techno-sharingï¼š@shoulder @ç‹é“éœ‡ @æ¨æ¬¢ @æ–°å¹´äº¬

Discussionï¼š@å‰ä»°è·³æŠ• @æµ®ç¾½ @MYX @Winniy @GUI @Planet

## âœ¨ Connection

![connect](./docs/assets/connect.png)

## Â©ï¸Â License

This project is licensed under the [CC-BY-4.0](https://creativecommons.org/licenses/by/4.0/deed.zh-hans) open-source license.




